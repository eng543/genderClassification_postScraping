{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams, trigrams\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import string\n",
    "import re\n",
    "#from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1 = pd.read_csv(\"scraped_tweets1.csv\")\n",
    "t2 = pd.read_csv(\"scraped_tweets2.csv\")\n",
    "t3 = pd.read_csv(\"scraped_tweets3.csv\")\n",
    "t4 = pd.read_csv(\"scraped_tweets4.csv\")\n",
    "t5 = pd.read_csv(\"scraped_tweets5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames = [t1, t2, t3, t4, t5]\n",
    "tweets = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove non-english tweets, reset indices\n",
    "tweets_eng = tweets[tweets['language'].isin(['en', 'und'])]\n",
    "tweets_eng = tweets_eng.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter users who have less than 100 tweets\n",
    "tweets_eng['count'] = 1\n",
    "tweet_count_grouped = tweets_eng.groupby('query')\n",
    "tweet_count = tweet_count_grouped['count'].agg([np.sum])\n",
    "tweet_count['keep'] = tweet_count['sum'] >= 100\n",
    "tweet_keep = tweet_count[tweet_count['keep'] == True]\n",
    "users_tweet_keep = list(tweet_keep.index)\n",
    "tweets_eng_keep = tweets_eng[tweets_eng['query'].isin(users_tweet_keep)]\n",
    "\n",
    "#len(tweets_eng_keep), len(tweets_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter users who have 3x as many friends as followers (potential bots)\n",
    "friend_count_grouped = tweets_eng_keep.groupby('query')\n",
    "followers_count = friend_count_grouped['num_followers'].agg([np.mean])\n",
    "friends_count = friend_count_grouped['num_friends'].agg([np.mean])\n",
    "friends_count['ratio'] = friends_count['mean']/friends_count['mean']\n",
    "friends_count['keep'] = friends_count['ratio'] <= 3\n",
    "ratio_keep = friends_count[friends_count['keep'] == True]\n",
    "users_ratio_keep = list(ratio_keep.index)\n",
    "tweets_trimmed = tweets_eng_keep[tweets_eng_keep['query'].isin(users_ratio_keep)]\n",
    "#len(tweets_trimmed), len(tweets_eng_keep), len(tweets_eng)\n",
    "tweets_trimmed = tweets_trimmed.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deal with NaN\n",
    "tweets_trimmed['entities_hashtags'] = tweets_trimmed['entities_hashtags'].fillna(0)\n",
    "tweets_trimmed['entities_urls'] = tweets_trimmed['entities_urls'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_subset1 = tweets_trimmed[tweets_trimmed['query'] == \"ForeverMoreVids\"]\n",
    "tweets_subset2 = tweets_trimmed[tweets_trimmed['query'] == \"brianjdixon\"]\n",
    "subsets = [tweets_subset1, tweets_subset2]\n",
    "tweets_subset = pd.concat(subsets)\n",
    "tweets_subset = tweets_subset.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# expressions to remove punctuation, urls, hashtags, @mentions, numbers, emoticons\n",
    "punctuation = set(string.punctuation)\n",
    "blank = [\"\", \" \"]\n",
    "url = re.compile(r'^http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+')\n",
    "hashtags = re.compile(r'^(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)')\n",
    "mentions = re.compile(r'^(?:@[\\w_]+)')\n",
    "numbers = re.compile(r'(\\d+)\\D*(\\d*)\\D*(\\d*)\\D*(\\d*)') # includes phone numbers\n",
    "smiley = re.compile(r'[:=;\\|\\)\\(\\[\\]\\{\\}][oO\\-]?[D\\)\\]\\(\\[/\\\\OpP\\|\\{\\}:]')\n",
    "remove_regex = [url, hashtags, mentions, numbers, smiley]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(strip_handles = True, reduce_len = True)\n",
    "p_stemmer = PorterStemmer()\n",
    "en_stop = [u'a', u'about', u'above', u'after', u'again', u'against', u'all', u'am', u'an', u'and', u'any', u'are', u\"aren't\", u'as', u'at', u'be', u'because', u'been', u'before', u'being', u'below', u'between', u'both', u'but', u'by', u\"can't\", u'cannot', u'could', u\"couldn't\", u'did', u\"didn't\", u'do', u'does', u\"doesn't\", u'doing', u\"don't\", u'down', u'during', u'each', u'few', u'for', u'from', u'further', u'had', u\"hadn't\", u'has', u\"hasn't\", u'have', u\"haven't\", u'having', u'he', u\"he'd\", u\"he'll\", u\"he's\", u'her', u'here', u\"here's\", u'hers', u'herself', u'him', u'himself', u'his', u'how', u\"how's\", u'i', u\"i'd\", u\"i'll\", u\"i'm\", u\"i've\", u'if', u'in', u'into', u'is', u\"isn't\", u'it', u\"it's\", u'its', u'itself', u\"let's\", u'me', u'more', u'most', u\"mustn't\", u'my', u'myself', u'no', u'nor', u'not', u'of', u'off', u'on', u'once', u'only', u'or', u'other', u'ought', u'our', u'ours', u'ourselves', u'out', u'over', u'own', u'same', u\"shan't\", u'she', u\"she'd\", u\"she'll\", u\"she's\", u'should', u\"shouldn't\", u'so', u'some', u'such', u'than', u'that', u\"that's\", u'the', u'their', u'theirs', u'them', u'themselves', u'then', u'there', u\"there's\", u'these', u'they', u\"they'd\", u\"they'll\", u\"they're\", u\"they've\", u'this', u'those', u'through', u'to', u'too', u'under', u'until', u'up', u'very', u'was', u\"wasn't\", u'we', u\"we'd\", u\"we'll\", u\"we're\", u\"we've\", u'were', u\"weren't\", u'what', u\"what's\", u'when', u\"when's\", u'where', u\"where's\", u'which', u'while', u'who', u\"who's\", u'whom', u'why', u\"why's\", u'with', u\"won't\", u'would', u\"wouldn't\", u'you', u\"you'd\", u\"you'll\", u\"you're\", u\"you've\", u'your', u'yours', u'yourself', u'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terms_all = []\n",
    "users = []\n",
    "\n",
    "for user in list(set(tweets_subset['query'])):\n",
    "    user_sub = tweets_subset[tweets_subset['query'] == user]\n",
    "    tweets = list(set(user_sub['content']))\n",
    "    # unigrams only\n",
    "    user_terms = []\n",
    "    for tweet in tweets:\n",
    "        terms = tknzr.tokenize(tweet.lower())\n",
    "        for term in terms:\n",
    "            if (not any(rr.search(term) for rr in remove_regex)) and (term not in punctuation) and not (term.startswith('www')):\n",
    "                if term not in en_stop:\n",
    "                    stemmed_term = p_stemmer.stem(term)\n",
    "                    user_terms.append(stemmed_term.encode('ascii', 'ignore'))\n",
    "    \n",
    "    terms_all.append(user_terms)\n",
    "    users.append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print terms_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(terms_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(term) for term in terms_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=1, id2word = dictionary, passes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldamodel.save('savedModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
